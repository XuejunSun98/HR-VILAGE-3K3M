[
  {
    "objectID": "Affymetrix_microarray_process.html",
    "href": "Affymetrix_microarray_process.html",
    "title": "\nAffymetrix Microarray Data Preprocessing\n",
    "section": "",
    "text": "Affymetrix Microarray Data Preprocessing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nXuejun Sun\n\n\n2025-05-03\n\n\n###loading packages\nlibrary(GEOquery)\nlibrary(ggplot2)\nlibrary(preprocessCore)\nlibrary(affy)     # For Affymetrix microarray data\nlibrary(HGNChelper)\nlibrary(limma)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(corrplot)\n### Set working directory and load raw data\nsetwd(\"/Users/xuejunsun/Library/CloudStorage/OneDrive-UniversityofNorthCarolinaatChapelHill/Study/2025 spring/RA/Data Preprocessing/Vaccine/GSE74817/GSE74816_RAW\")  # Set directory to where CEL files \nraw_data &lt;- ReadAffy()\nnormalized_data &lt;- rma(raw_data, normalize = FALSE)\n## Background correcting\n## Calculating Expression\nexpr_matrix &lt;- exprs(normalized_data)\ncolnames(expr_matrix) &lt;- sub(\"_.*\", \"\", colnames(expr_matrix))  # Remove suffix after \"_\"\n### Convert to data frame\nexpr_matrix &lt;- as.data.frame(expr_matrix)\n\n### Map probe IDs to gene symbols\ngeo &lt;- getGEO(\"GSE74816\", GSEMatrix = TRUE)[[1]]\nfeatures &lt;- fData(geo)[, c(\"ID\", \"Gene Symbol\")]\nfeatures$`Gene Symbol` &lt;- sub(\"///.*\", \"\", features$`Gene Symbol`)\nfeatures$Symbol &lt;- checkGeneSymbols(features$`Gene Symbol`)$Suggested.Symbol\nfeatures &lt;- features[, c(\"ID\", \"Symbol\")]\n\n### Join gene symbols and filter by highest median expression\nexpr_matrix$ID &lt;- rownames(expr_matrix)\nexpr_matrix &lt;- left_join(expr_matrix, features, by = \"ID\") %&gt;%\n  select(-ID) %&gt;%\n  filter(!is.na(Symbol)) %&gt;%\n  rowwise() %&gt;%\n  mutate(RowMedian = median(c_across(where(is.numeric)), na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  group_by(Symbol) %&gt;%\n  slice_max(RowMedian, with_ties = FALSE) %&gt;%\n  ungroup() %&gt;%\n  select(-RowMedian)%&gt;%\n  as.data.frame()\n\n### Final formatting\nexpr_matrix &lt;- na.omit(expr_matrix)\nrownames(expr_matrix) &lt;- expr_matrix$Symbol\nexpr_matrix &lt;- expr_matrix[, -which(names(expr_matrix) == \"Symbol\")]\n\n#write.csv(expr_matrix,\"~/GSE74816_gene_expr.csv\")\n\n### Quantile normalization\ndata &lt;- normalize.quantiles(as.matrix(expr_matrix))\ndata &lt;- as.data.frame(data)\nrownames(data) &lt;- rownames(expr_matrix)\ncolnames(data) &lt;- colnames(expr_matrix)\n\n###save data\n#write.csv(data,\"~/GSE74816_gene_expr_processed.csv\")\n\n\nget meta data from GEO for visualization\n\nmeta&lt;-geo@phenoData@data\nsum(rownames(meta)==colnames(data))\n## [1] 177\n\n\n\nVisualize processed data\n\n###box plot of normalized samples\n\ndata_long &lt;- pivot_longer(data[,1:50], cols = everything(), names_to = \"Variable\", values_to = \"Value\")\n\nggplot(data_long, aes(x = Variable, y =Value)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"TRUE\" = \"red\", \"FALSE\" = \"gray\")) +\n  theme_minimal() +\n  labs(title = \"Box Plot of QN samples\",\n       x = \"Variable\",\n       y = \"Value\") +\n  theme(legend.position = \"none\")\n\n\n\n# Correlation plot\nm&lt;-cor(data,method = \"spearman\")\n\ncorrplot(m, tl.pos = \"n\",method = \"color\", col.lim = c(0.8, 1), col = COL1('Blues'),is.corr = FALSE, )\n\n\n\n# PCA plot\npca_result &lt;- prcomp(t(data), scale. = F)  # Scale features\n# Extract PCA results\npca_scores &lt;- as.data.frame(pca_result$x)  # PCA scores (PC1, PC2, etc.)\npca_scores &lt;- cbind(pca_scores, meta)  # Add covariates\n# PCA by observed time (continuous variable), there is some trend,using model to test\nggplot(pca_scores, aes(x = PC1, y = PC2,color=`time point:ch1`)) +\n    geom_point(size = 3) +\n    labs(title = \"PCA by timepoint\", x = \"PC1\", y = \"PC2\") +\n    theme_minimal()",
    "crumbs": [
      "Home",
      "Data Preprocessing",
      "Affymetrix Microarray Process"
    ]
  },
  {
    "objectID": "Illumina_microarray_process.html",
    "href": "Illumina_microarray_process.html",
    "title": "\nIllumina Microarray Process\n",
    "section": "",
    "text": "Illumina Microarray Process\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nXuejun Sun\n\n\n2025-05-03\n\n\n###loading packages\nlibrary(GEOquery)\nlibrary(ggplot2)\nlibrary(preprocessCore)\nlibrary(affy)     # For Affymetrix microarray data\nlibrary(HGNChelper)\nlibrary(limma)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(corrplot)\n### Convert to data frame\nexpr_matrix &lt;-read.csv(\"/Users/xuejunsun/Library/CloudStorage/OneDrive-UniversityofNorthCarolinaatChapelHill/Study/Dissertation/Project2/data/gene_expr_raw2/GSE101710_raw.csv\",row.names = \"X\")\n\n### Map probe IDs to gene symbols\ngeo &lt;- getGEO(\"GSE101710\", GSEMatrix = TRUE)[[1]]\nfeatures &lt;- fData(geo)[, c(\"ID\", \"ILMN_Gene\")]\nfeatures$Symbol &lt;- checkGeneSymbols(features$ILMN_Gene)$Suggested.Symbol\nfeatures &lt;- features[, c(\"ID\", \"Symbol\")]\n\n### Join gene symbols and filter by highest median expression\nexpr_matrix$ID &lt;- rownames(expr_matrix)\nexpr_matrix &lt;- left_join(expr_matrix, features, by = \"ID\") %&gt;%\n  select(-ID) %&gt;%\n  filter(!is.na(Symbol)) %&gt;%\n  rowwise() %&gt;%\n  mutate(RowMedian = median(c_across(where(is.numeric)), na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  group_by(Symbol) %&gt;%\n  slice_max(RowMedian, with_ties = FALSE) %&gt;%\n  ungroup() %&gt;%\n  select(-RowMedian)%&gt;%\n  as.data.frame()\n\n### Final formatting\nexpr_matrix &lt;- na.omit(expr_matrix)\nrownames(expr_matrix) &lt;- expr_matrix$Symbol\nexpr_matrix &lt;- expr_matrix[, -which(names(expr_matrix) == \"Symbol\")]\n\n#write.csv(expr_matrix,\"~/GSE101710_gene_expr.csv\")\n\n### Quantile normalization\ndata &lt;- normalize.quantiles(as.matrix(expr_matrix))\ndata &lt;- as.data.frame(data)\nrownames(data) &lt;- rownames(expr_matrix)\ncolnames(data) &lt;- colnames(expr_matrix)\n\n###save data\n#write.csv(data,\"~/GSE101710_gene_expr_processed.csv\")\n\n\nget meta data from GEO for visualization\n\nmeta&lt;-geo@phenoData@data\nsum(rownames(meta)==colnames(data))\n## [1] 79\n\n\n\nVisualize processed data\n\n###box plot of normalized samples\n\ndata_long &lt;- pivot_longer(data[,1:50], cols = everything(), names_to = \"Variable\", values_to = \"Value\")\n\nggplot(data_long, aes(x = Variable, y =log2(Value+1))) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"TRUE\" = \"red\", \"FALSE\" = \"gray\")) +\n  theme_minimal() +\n  labs(title = \"Box Plot of QN samples\",\n       x = \"Variable\",\n       y = \"Value\") +\n  theme(legend.position = \"none\")\n\n\n\n# Correlation plot\nm&lt;-cor(data,method = \"spearman\")\n\ncorrplot(m, tl.pos = \"n\",method = \"color\", col.lim = c(0.8, 1), col = COL1('Blues'),is.corr = FALSE, )\n\n\n\n# PCA plot\npca_result &lt;- prcomp(t(data), scale. = F)  # Scale features\n# Extract PCA results\npca_scores &lt;- as.data.frame(pca_result$x)  # PCA scores (PC1, PC2, etc.)\npca_scores &lt;- cbind(pca_scores, meta)  # Add covariates\n# PCA by observed time (continuous variable), there is some trend,using model to test\nggplot(pca_scores, aes(x = PC1, y = PC2,color=`blood draw date:ch1`)) +\n    geom_point(size = 3) +\n    labs(title = \"PCA by timepoint\", x = \"PC1\", y = \"PC2\") +\n    theme_minimal()",
    "crumbs": [
      "Home",
      "Data Preprocessing",
      "Illumina Microarray Process"
    ]
  },
  {
    "objectID": "Antibody_Responder_Prediction.html",
    "href": "Antibody_Responder_Prediction.html",
    "title": "\nAntibody Responder Prediction Using Deep Learning Models¬∂\n",
    "section": "",
    "text": "Antibody Responder Prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis notebook aims to benchmark longitudinal prediction models for immune response using time-series gene expression data derived from human viral vaccination studies. We implement and evaluate several deep learning models:\n\n\n\nLogistic regression with PCA-reduced features and timepoint as predictor\n\n\nSimpleRNN\n\n\nGRU\n\n\nLSTM\n\n\nTransformer\n\n\n\nThese models are tested across three different batch correction methods:\n\n\n\nQuantile Normalization (QN)\n\n\nRegression-based residuals\n\n\nComBat\n\n\n\nAll models are assessed using stratified 5-fold cross-validation, and evaluated with:\n\n\n\nAccuracy\n\n\nF1-score\n\n\nAUC (Area Under the ROC Curve)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSection 1: Data Loading and Preprocessing¬∂\n\n\nWe begin by loading the preprocessed gene expression matrices and associated sample metadata. Key preprocessing steps include:\n\n\n\nBy customized criteria load data from HR-VILAGE-3K3M Repo\n\n\nLog2 transformation of RNA-seq samples\n\n\nQuantile normalization across samples\n\n\nFiltering genes with low mean and variance (bottom 25%)\n\n\nAligning metadata (combined_ID, timepoint_numeric, and responder2) to expression data\n\n\n\nThis ensures consistency across all input formats for both deep learning and traditional baseline models.\n\n\n\n\n\n\n\n\n\n\n\n\nIn¬†[1]:\n\n\n\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\nfrom huggingface_hub import login\nfrom datasets import load_dataset\nimport pandas as pd\n\n# Define variables\nrepo_id = \"xuejun72/HR-VILAGE-3K3M\"  # Your dataset repo on Hugging Face\n\nstudy_meta = load_dataset(repo_id, data_files = \"study_meta.csv\")[\"train\"].to_pandas()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/users/x/u/xuejun1/HR_VILAGE/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nGenerating train split: 66 examples [00:00, 10109.34 examples/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample Inclusion Criteria: We curated the dataset to include only influenza or influenza/pneumococcal vaccine studies with recorded antibody responses. Samples were limited to those collected before day 28 post-vaccination to capture early immune responses. Subjects receiving placebo or control vaccines (e.g., Saline, Pneumovax) were excluded. We binarized the antibody response into High-responder vs.¬†Low-responder (combining Moderate and Non-responders) and removed studies with severely unbalanced class distributions. A unique combined_ID was generated for each subject to enable longitudinal modeling.\n\n\n\n\n\n\n\n\n\n\n\n\nIn¬†[2]:\n\n\n\n\n# === Step 0: Select study_IDs based on inclusion criteria ===\n\n# === Step 1: Load and subset metadata to selected study_IDs ===\nsplit_filter = {\"study_type\": [\"vaccine\",\"mixed exposure\"], \"platform\": [], \"tissue\": [], \"pathogen\": [\"influenza\",\"influenza/pneumococcal\"],\"antibody\": [\"yes\"]}\nmeta_df = load_dataset(repo_id, name = \"meta\", trust_remote_code=True, split_filter=split_filter, revision=\"script\")\n\nfor _, value in meta_df.items(): \n    meta_df = value.to_pandas().set_index(\"row_name\")\n\nfiltered_meta_df = meta_df\n\n# === Step 2: Remove rows without a recorded responder status ===\nfiltered_meta_df = filtered_meta_df.dropna(subset=['responder']).copy()\n\n# === Step 3: Collapse responder into binary responder2: \n# 'High-responder' vs. 'Low-responder' (includes Moderate + Non)\nfiltered_meta_df['responder2'] = filtered_meta_df['responder'].replace({\n    'High-responder': 'High-responder',\n    'Moderate-responder': 'Low-responder',\n    'Non-responder': 'Low-responder'\n})\n\n# === Step 4: Print responder2 counts per study before further exclusions ===\nprint(\"\\n‚úÖ Responder2 distribution by study (before exclusions):\")\ninitial_summary = pd.crosstab(filtered_meta_df[\"study_ID\"], filtered_meta_df[\"responder2\"])\nprint(initial_summary)\n\n# === Step 5: Exclude studies with severely unbalanced or problematic class distributions ===\nexcluded_studies = ['GSE29615', 'GSE45735', 'GSE74815']\nfiltered_meta = filtered_meta_df[~filtered_meta_df[\"study_ID\"].isin(excluded_studies)].copy()\n\n# === Step 6: Remove subjects that received control/placebo vaccines (e.g., Pneumovax, Saline) ===\nfiltered_meta = filtered_meta[~filtered_meta['vaccine'].isin(['Pneumovax', 'Saline'])].copy()\n\n# === Step 7: Convert 'timepoint' column to numeric to allow numeric comparisons (e.g., \"0\", \"7\", etc.) ===\nfiltered_meta['timepoint_numeric'] = pd.to_numeric(filtered_meta['timepoint'], errors='coerce')\n\n# === Step 8: Create a unique subject ID by concatenating study_ID and individual ID ===\nfiltered_meta['combined_ID'] = filtered_meta['study_ID'].astype(str) + \"_\" + filtered_meta['ID'].astype(str)\n\n# === Step 9: Restrict to timepoints that occur before day 28 (early immune response period) ===\nfiltered_meta = filtered_meta[filtered_meta['timepoint_numeric'] &lt; 28].copy()\n\n# === Step 10: Print final summary of responder2 counts per study after all filters ===\nprint(\"\\n‚úÖ Responder2 distribution by study (after filtering):\")\nfinal_summary = pd.crosstab(filtered_meta[\"study_ID\"], filtered_meta[\"responder2\"])\nprint(final_summary)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:00&lt;00:00, 65.51files/s]\nGenerating vaccine.mixed_exposure_influenza.influenza_pneumococcal_yes split: 836 examples [00:00, 7776.51 examples/s]\n\n\n\n\n\n\n\nLoading GSE194378...\n\nLoading GSE74811...\n\nLoading GSE74813...\n\nLoading GSE74815...\n\nLoading GSE74816...\n\nLoading GSE48023...\n\nLoading GSE48018...\n\n\n\n\n\n\n\n\nGenerating vaccine.mixed_exposure_influenza.influenza_pneumococcal_yes split: 2990 examples [00:00, 6982.14 examples/s]\n\n\n\n\n\n\n\nLoading GSE41080...\n\nLoading GSE59743...\n\nLoading GSE59654...\n\nLoading GSE101709...\n\nLoading GSE59635...\n\nLoading GSE101710...\n\nLoading GSE47353...\n\nLoading SDY311...\n\nLoading SDY112...\n\nLoading SDY315...\n\nLoading GSE207750...\n\n\n\n\n\n\n\n\nGenerating vaccine.mixed_exposure_influenza.influenza_pneumococcal_yes split: 4158 examples [00:00, 6792.73 examples/s]\n\n\n\n\n\n\n\n\nLoading GSE29614...\n\nLoading GSE29615...\n\nLoading GSE29617...\n\nLoading GSE48762...\n\nLoading GSE45735...\n\n‚úÖ Responder2 distribution by study (before exclusions):\nresponder2  High-responder  Low-responder\nstudy_ID                                 \nGSE101709               42             56\nGSE101710               59             20\nGSE194378              228            154\nGSE207750               78            193\nGSE29614                21              6\nGSE29615                 3             80\nGSE29617                59             21\nGSE41080                56             26\nGSE45735                54              0\nGSE47353               229             55\nGSE48018               412             14\nGSE48023               386             31\nGSE48762               191            178\nGSE59635                42             30\nGSE59654                56            100\nGSE59743                56             64\nGSE74811                48             35\nGSE74813               233             48\nGSE74815                66              9\nGSE74816                82             95\nSDY112                  59             30\nSDY311                  48             22\nSDY315                  29             40\n\n‚úÖ Responder2 distribution by study (after filtering):\nresponder2  High-responder  Low-responder\nstudy_ID                                 \nGSE101709               31             42\nGSE101710               45             16\nGSE194378              183            123\nGSE207750               78            193\nGSE29614                21              6\nGSE29617                59             21\nGSE41080                56             26\nGSE47353               184             43\nGSE48018               412             14\nGSE48023               386             31\nGSE48762               106             54\nGSE59635                32             23\nGSE59654                43             75\nGSE59743                42             48\nGSE74811                48             35\nGSE74813               233             48\nGSE74816                82             95\nSDY112                  59             30\nSDY311                  46             21\nSDY315                  29             40\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn¬†[4]:\n\n\n\n\nimport time\n\n\n# Start timer\nstart_time = time.time()\n\ngene_expr_df = load_dataset(repo_id, name = \"bulk_gene_expr\", trust_remote_code=True, split_filter=split_filter, revision=\"script\")\ngene_names = gene_expr_df[\"gene_expr_colnames\"][0][\"gene_names\"]\n\nall_row_names = []\nall_matrix = []\nfor batch in next(iter(gene_expr_df.items()))[1]:\n    all_row_names.extend(batch[\"row_names\"])\n    all_matrix.extend(batch[\"matrix\"])\ngene_expr_df = pd.DataFrame(all_matrix, index=all_row_names, columns=gene_names)\ngene_expr_df\n\n\n# End timer\nend_time = time.time()\nelapsed_minutes = (end_time - start_time) / 60\n\nprint(f\"‚úÖ gene_expr loaded with shape: {gene_expr_df.shape}\")\nprint(f\"‚è±Ô∏è Time taken to load gene_expr: {elapsed_minutes:.2f} minutes\")\n\n# Use the index of filtered_meta directly\nmatching_ids = filtered_meta.index\n\n# Filter gene_expr_df to keep only matching rownames (GSM IDs)\nfiltered_gene_expr_df = gene_expr_df.loc[gene_expr_df.index.intersection(matching_ids)]\n# Drop columns (genes) that contain any NaNs\nfiltered_gene_expr_df = filtered_gene_expr_df.dropna(axis=1)\n\nprint(f\"‚úÖ Filtered gene_expr shape: {filtered_gene_expr_df.shape}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚úÖ gene_expr loaded with shape: (4158, 41667)\n‚è±Ô∏è Time taken to load gene_expr: 1.34 minutes\n‚úÖ Filtered gene_expr shape: (3159, 14554)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSection 2: Batch Effect Correction¬∂\n\n\nTo address technical variability across studies, we applied three normalization strategies. First, RNA-seq samples were log-transformed using log2(x + 1), and genes with low mean and variance (bottom 25%) were removed. Then, we applied Quantile Normalization (QN), which aligns the distribution of gene expression values across samples by assigning the average value at each rank, ensuring all samples share the same empirical distribution. Next, we applied ComBat, an empirical Bayes method that adjusts for known batch effects (study ID), and regression-based adjustment, where study-specific effects were regressed out using scanpy‚Äôs regress_out function. These approaches yielded three harmonized datasets (QN, ComBat, Regression) for downstream modeling and comparison.\n\n\n\n\n\n\n\n\n\n\n\n\nIn¬†[5]:\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport scanpy as sc\n\n# Copy expression and metadata\ndf = filtered_gene_expr_df.copy()\nmeta = filtered_meta.copy()\n\n# Apply log2(x + 1) transform to RNA-seq samples\nrna_seq_samples = meta[meta[\"platform\"].str.lower() == \"rna-seq\"].index\ndf.loc[rna_seq_samples] = np.log2(df.loc[rna_seq_samples] + 1)\n\n# Remove low mean and low variance genes\ngene_means = df.mean(axis=0)\ngene_vars = df.var(axis=0)\n\n# Determine 25th percentile thresholds\nmean_thresh = gene_means.quantile(0.25)\nvar_thresh = gene_vars.quantile(0.25)\n\n# Filter: keep genes above both thresholds\ngenes_to_keep = gene_means[(gene_means &gt; mean_thresh) & (gene_vars &gt; var_thresh)].index\ndf = df[genes_to_keep]\n\n#  R-style quantile normalization (transpose aware)\ndef quantile_normalize_matrix_rows_are_samples(expr_df):\n\n    df_t = expr_df.T  # Now genes are rows, samples are columns\n\n    # Sort each column (i.e., each sample)\n    sorted_mat = np.sort(df_t.values, axis=0)\n\n    # Compute mean of sorted values (average across samples for each rank)\n    mean_ranks = np.mean(sorted_mat, axis=1)\n\n    # Get rank indices for each gene in each sample\n    ranks = df_t.rank(method=\"min\", axis=0).astype(int) - 1\n\n    # Replace values in original order based on rank\n    norm_df_t = df_t.copy()\n    for col in df_t.columns:\n        norm_df_t[col] = [mean_ranks[r] for r in ranks[col].values]\n\n    return norm_df_t.T  # Return to (samples x genes) shape\n\n# Apply quantile normalization\nqn_expr_df = quantile_normalize_matrix_rows_are_samples(df)\n\nprint(f\"\\n‚úÖ Quantile-normalized expression shape: {qn_expr_df.shape}\")\n\nadata = sc.AnnData(df)\n\n# Add batch information\nadata.obs[\"batch\"] = filtered_meta.loc[adata.obs_names, \"study_ID\"]\n\n# Apply ComBat\nsc.pp.combat(adata, key=\"batch\")\n\ncombat_expr_df = pd.DataFrame(adata.X, index=adata.obs_names, columns=adata.var_names)\n\n# Apply ComBat Regress out\nsc.pp.regress_out(adata, keys=['batch'])\n\n# Step 5: Convert back to DataFrame\nregressed_expr_df = pd.DataFrame(adata.X, index=adata.obs_names, columns=adata.var_names)\nprint(combat_expr_df.shape)\nprint(regressed_expr_df.shape)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚úÖ Quantile-normalized expression shape: (3159, 7276)\n\n\n\n\n\n\n\n\n/users/x/u/xuejun1/HR_VILAGE/lib/python3.11/site-packages/scanpy/preprocessing/_combat.py:347: RuntimeWarning: divide by zero encountered in divide\n  (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()\n\n\n\n\n\n\n\n\n(3159, 7276)\n(3159, 7276)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn¬†[6]:\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\n\n# 1. Match metadata to expression data\ndef match_meta(expr, meta):\n    return meta.loc[expr.index]\n\ncombat_meta = match_meta(combat_expr_df, filtered_meta)\nqn_meta = match_meta(qn_expr_df, filtered_meta)\nreg_meta = match_meta(regressed_expr_df, filtered_meta)\n\n# 2. Run t-SNE\ndef run_tsne(X, random_state=42):\n    tsne = TSNE(\n        n_components=2,\n        random_state=random_state,\n        perplexity=30,\n        max_iter=1000  \n    )\n    return tsne.fit_transform(X)\ncombat_meta[['TSNE1', 'TSNE2']] = run_tsne(combat_expr_df)\nqn_meta[['TSNE1', 'TSNE2']] = run_tsne(qn_expr_df)\nreg_meta[['TSNE1', 'TSNE2']] = run_tsne(regressed_expr_df)\n\n# 3. Define color palettes\npalette_outcome = sns.color_palette(\"Set1\", n_colors=2)      # responder2\npalette_studyid = sns.color_palette(\"tab20\", n_colors=20)    # study_ID\n\n# 4. Plotting with save option\ndef plot_tsne_side_by_side(meta, method_name, save_path=None):\n    fig, axes = plt.subplots(1, 2, figsize=(14, 10))\n\n    # Plot 1: By responder2\n    scatter1 = sns.scatterplot(\n        data=meta,\n        x='TSNE1', y='TSNE2',\n        hue='responder2',\n        palette=palette_outcome,\n        s=50, alpha=0.7,\n        ax=axes[0]\n    )\n    axes[0].set_title(f\"{method_name}\", fontsize=25)\n    axes[0].set_xlabel(\"t-SNE 1\", fontsize=20)\n    axes[0].set_ylabel(\"t-SNE 2\", fontsize=20)\n    axes[0].tick_params(labelsize=16)\n\n    # Plot 2: By study_ID\n    scatter2 = sns.scatterplot(\n        data=meta,\n        x='TSNE1', y='TSNE2',\n        hue='study_ID',\n        palette=palette_studyid,\n        s=60, alpha=0.7,\n        ax=axes[1]\n    )\n    axes[1].set_title(f\"{method_name}\", fontsize=30)\n    axes[1].set_xlabel(\"t-SNE 1\", fontsize=30)\n    axes[1].set_ylabel(\"t-SNE 2\", fontsize=30)\n    axes[1].tick_params(labelsize=25)\n\n    # Remove individual legends\n    axes[0].legend_.remove()\n    axes[1].legend_.remove()\n\n    # Add shared legends\n    handles1, labels1 = scatter1.get_legend_handles_labels()\n    handles2, labels2 = scatter2.get_legend_handles_labels()\n\n    fig.legend(handles1, labels1, title='Responder', title_fontsize=14, fontsize=12,\n               loc='center left', bbox_to_anchor=(1.01, 1))\n    fig.legend(handles2, labels2, title='Study ID', title_fontsize=14, fontsize=12,\n               loc='center left', bbox_to_anchor=(1.01, 0.5))\n\n    plt.tight_layout()\n\n    if save_path:\n        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n        print(f\"‚úÖ Figure saved to {save_path}\")\n\n    plt.show()\n\n# 5. Generate and save plots\nplot_tsne_side_by_side(qn_meta, \"Quantile Normalization\", save_path=\"tsne_QN.png\")\nplot_tsne_side_by_side(reg_meta, \"Regression\", save_path=\"tsne_Regressed.png\")\nplot_tsne_side_by_side(combat_meta, \"ComBat\", save_path=\"tsne_ComBat.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚úÖ Figure saved to tsne_QN.png\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚úÖ Figure saved to tsne_Regressed.png\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚úÖ Figure saved to tsne_ComBat.png\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSection 3: Model Fitting¬∂\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSequence Construction and Padding: To model temporal gene expression dynamics, we transformed each subject‚Äôs longitudinal data into a sequence format by concatenating gene expression vectors with their corresponding timepoints. This was done separately for each normalization method (Quantile Normalization, ComBat, and Regression-adjusted data). The sequences were padded with zeros to match the length of the longest sequence across all subjects, ensuring compatibility with deep learning models that require fixed-length inputs. Each padded dataset was accompanied by binary response labels indicating high or low antibody responders.\n\n\n\n\n\n\n\n\n\n\n\n\nIn¬†[7]:\n\n\n\n\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Function to generate sequence data\ndef generate_sequence_data(expr_df, combined_meta, subject_label):\n    X_seq = []\n    y_seq = []\n    max_seq_len = 0\n\n    for subject_id, df_sub in tqdm(combined_meta.groupby('combined_ID'), desc=\"Building sequences\"):\n        sample_ids = df_sub.sort_values('timepoint_numeric').index  # sorted timepoints\n\n        try:\n            timepoints = df_sub.loc[sample_ids, 'timepoint_numeric'].values[:, np.newaxis]  # (T, 1)\n            expr = expr_df.loc[sample_ids].values  # (T, G)\n\n            # Combine expression and time\n            features = np.concatenate([expr, timepoints], axis=1)  # (T, G+1)\n            X_seq.append(features)\n            y_seq.append(subject_label.loc[subject_id])\n            max_seq_len = max(max_seq_len, features.shape[0])\n        except KeyError:\n            continue\n\n    return X_seq, y_seq, max_seq_len\n\n# Prepare labels\nsubject_label = filtered_meta.drop_duplicates('combined_ID').set_index('combined_ID')['responder2']\nsubject_label = subject_label.map({'High-responder': 1, 'Low-responder': 0})\n\n# Generate sequences for each normalization method\nX_seq_qn, y_seq_qn, max_len_qn = generate_sequence_data(qn_expr_df, filtered_meta, subject_label)\nX_seq_cb, y_seq_cb, max_len_cb = generate_sequence_data(combat_expr_df, filtered_meta, subject_label)\nX_seq_reg, y_seq_reg, max_len_reg = generate_sequence_data(regressed_expr_df, filtered_meta, subject_label)\n\n# Determine global maximum sequence length\nmax_seq_len_all = max(max_len_qn, max_len_cb, max_len_reg)\n\n# Pad sequences using GPU-enabled TensorFlow\nX_padded_qn = pad_sequences(X_seq_qn, maxlen=max_seq_len_all, padding='post', dtype='float32')\nX_padded_cb = pad_sequences(X_seq_cb, maxlen=max_seq_len_all, padding='post', dtype='float32')\nX_padded_reg = pad_sequences(X_seq_reg, maxlen=max_seq_len_all, padding='post', dtype='float32')\n\n# Convert labels to arrays\ny_array_qn = np.array(y_seq_qn)\ny_array_cb = np.array(y_seq_cb)\ny_array_reg = np.array(y_seq_reg)\n\n# Output shapes\nprint(f\"QN padded shape:        {X_padded_qn.shape}, Labels: {y_array_qn.shape}\")\nprint(f\"ComBat padded shape:    {X_padded_cb.shape}, Labels: {y_array_cb.shape}\")\nprint(f\"Regressed padded shape: {X_padded_reg.shape}, Labels: {y_array_reg.shape}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2025-05-14 21:33:05.378088: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-05-14 21:33:08.311671: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nBuilding sequences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1268/1268 [00:00&lt;00:00, 2189.92it/s]\nBuilding sequences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1268/1268 [00:00&lt;00:00, 1835.91it/s]\nBuilding sequences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1268/1268 [00:00&lt;00:00, 1800.99it/s]\n\n\n\n\n\n\n\n\nQN padded shape:        (1268, 19, 7277), Labels: (1268,)\nComBat padded shape:    (1268, 19, 7277), Labels: (1268,)\nRegressed padded shape: (1268, 19, 7277), Labels: (1268,)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA + Logistic Regression Model: To establish a baseline for longitudinal prediction, we implemented a logistic regression model using principal components derived from gene expression data. Specifically, we performed PCA on the normalized gene expression matrix and selected the top 10 principal components as input features, which together explained over 80% of the total variance‚Äîensuring the major biological signal was retained while reducing dimensionality and multicollinearity. The numeric timepoint of each sample was included as an additional covariate.\n\n\n\n\n\n\n\n\n\n\n\n\nIn¬†[8]:\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport gc\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\nimport patsy\n\ndef run_logistic_pipeline(expr_df, meta_df, method_name, seed_list = [2368, 5272, 9289, 1251, 8825, 355]):\n    results = []\n\n    # 1. Ensure matching index\n    assert all(expr_df.index == meta_df.index), \"Mismatch in sample order\"\n\n    # 2. Binary response\n    y_array = meta_df[\"responder2\"].map({'High-responder': 1, 'Low-responder': 0}).values\n    y_array = pd.to_numeric(y_array, errors=\"coerce\")\n\n    # 3. PCA transformation\n    pca = PCA(n_components=100)\n    X_pca = pca.fit_transform(expr_df.values)\n\n    # 4. Time feature scaling\n    time_feature = meta_df[\"timepoint_numeric\"].astype(float).values.reshape(-1, 1)\n    time_feature = StandardScaler().fit_transform(time_feature)\n\n    # 5. Combine features\n    X_final = np.concatenate([X_pca, time_feature], axis=1)\n    columns = [f\"PC{i+1}\" for i in range(X_pca.shape[1])] + [\"Time\"]\n    full_df = pd.DataFrame(X_final, columns=columns)\n    full_df[\"y\"] = y_array\n\n    # 6. Stratified Cross-validation\n    for seed in seed_list:\n        print(f\"\\n=== {method_name} | Seed {seed} ===\")\n        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\n        for fold, (train_idx, test_idx) in enumerate(skf.split(X_final, y_array)):\n            print(f\"  Fold {fold+1}\")\n            train_df = full_df.iloc[train_idx].copy()\n            test_df = full_df.iloc[test_idx].copy()\n\n            try:\n                # Design matrices for logistic regression\n                formula = \"y ~ \" + \" + \".join(columns)\n                y_train, X_train = patsy.dmatrices(formula, data=train_df, return_type='dataframe')\n                y_train = y_train.values.ravel()\n\n                # Fit logistic regression\n                model = sm.Logit(y_train, X_train)\n                result = model.fit(disp=False)\n\n                # Predict on test set\n                X_test = patsy.dmatrix(\" + \".join(columns), data=test_df, return_type='dataframe')\n                y_pred_prob = result.predict(X_test)\n                y_pred = (y_pred_prob &gt; 0.5).astype(int)\n\n                acc = accuracy_score(test_df[\"y\"], y_pred)\n                f1 = f1_score(test_df[\"y\"], y_pred)\n                auc = roc_auc_score(test_df[\"y\"], y_pred_prob)\n\n                results.append({\n                    \"method\": method_name,\n                    \"seed\": seed,\n                    \"fold\": fold + 1,\n                    \"accuracy\": acc,\n                    \"f1\": f1,\n                    \"auc\": auc\n                })\n\n\n            except Exception as e:\n                print(f\"‚ö†Ô∏è {method_name} failed on seed {seed}, fold {fold+1}: {e}\")\n                continue\n\n            gc.collect()\n\n    return pd.DataFrame(results)\n\n# === Run Logistic Regression for all normalization methods ===\nresults_qn  = run_logistic_pipeline(qn_expr_df, filtered_meta, \"QN\")\nresults_cb  = run_logistic_pipeline(combat_expr_df, filtered_meta, \"ComBat\")\nresults_reg = run_logistic_pipeline(regressed_expr_df, filtered_meta, \"Regression\")\n\n# === Combine and save results ===\nall_results = pd.concat([results_qn, results_cb, results_reg], ignore_index=True)\nall_results.to_csv(\"logistic_regression_results.csv\", index=False)\n\n# === Print results ===\nprint(\"\\n‚úÖ Combined Logistic Regression Results:\")\nprint(all_results.sort_values([\"method\", \"seed\", \"fold\"]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n=== QN | Seed 2368 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== QN | Seed 5272 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== QN | Seed 9289 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== QN | Seed 1251 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== QN | Seed 8825 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== QN | Seed 355 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== ComBat | Seed 2368 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== ComBat | Seed 5272 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== ComBat | Seed 9289 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== ComBat | Seed 1251 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== ComBat | Seed 8825 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== ComBat | Seed 355 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== Regression | Seed 2368 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== Regression | Seed 5272 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== Regression | Seed 9289 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== Regression | Seed 1251 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== Regression | Seed 8825 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n=== Regression | Seed 355 ===\n  Fold 1\n  Fold 2\n  Fold 3\n  Fold 4\n  Fold 5\n\n‚úÖ Combined Logistic Regression Results:\n        method  seed  fold  accuracy        f1       auc\n55      ComBat   355     1  0.672468  0.792793  0.568178\n56      ComBat   355     2  0.681962  0.801579  0.616722\n57      ComBat   355     3  0.655063  0.781124  0.576218\n58      ComBat   355     4  0.688291  0.803980  0.580221\n59      ComBat   355     5  0.687797  0.804757  0.614051\n..         ...   ...   ...       ...       ...       ...\n70  Regression  9289     1  0.664557  0.785859  0.604831\n71  Regression  9289     2  0.675633  0.797230  0.584608\n72  Regression  9289     3  0.696203  0.812133  0.578844\n73  Regression  9289     4  0.674051  0.795635  0.603851\n74  Regression  9289     5  0.676704  0.797217  0.583779\n\n[90 rows x 6 columns]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRNN, LSTM, and GRU Models: To capture temporal patterns in the longitudinal gene expression data, we implemented three deep learning sequence models: SimpleRNN, LSTM, and GRU. Each model was designed with a single recurrent layer of 64 hidden units followed by a dropout layer (rate = 0.4) to reduce overfitting and a final dense sigmoid output layer for binary classification. We used a masking layer to handle varying sequence lengths and padded all sequences to a unified maximum time length. The models were trained with the Adam optimizer (learning rate = 1e-4), binary cross-entropy loss, and a batch size of 64 for 20 epochs. To ensure robust evaluation, stratified 5-fold cross-validation was repeated across six random seeds. These hyperparameters were chosen based on common practice in biomedical time-series modeling and to maintain consistency across architectures for fair comparison across batch correction methods (Quantile Normalization, ComBat, and Regression).\n\n\n\n\n\n\n\n\n\n\n\n\nIn¬†[9]:\n\n\n\n\nimport os\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport gc\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom tensorflow.keras import backend as K\n\n\n# === RNN Model Builder ===\ndef build_rnn_model(input_dim):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Masking(mask_value=0.0, input_shape=(None, input_dim)),\n        tf.keras.layers.SimpleRNN(64, return_sequences=False),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# === Cross-Validation Runner ===\ndef run_rnn_cv(X, y, label, out_csv=None):\n    seed_list = [2368, 5272, 9289, 1251, 8825, 355]\n    all_results = []\n\n    for seed in seed_list:\n        print(f\"\\n=== {label} | Seed {seed} ===\")\n        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\n        for fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n            print(f\"  Fold {fold+1}\")\n\n            X_train, X_test = X[train_idx], X[test_idx]\n            y_train, y_test = y[train_idx], y[test_idx]\n\n            model = build_rnn_model(input_dim=X.shape[2])\n            model.fit(\n                X_train, y_train,\n                epochs=20,\n                batch_size=64,\n                validation_split=0.2,\n                verbose=0\n            )\n\n            y_pred_prob = model.predict(X_test, verbose=0).ravel()\n            y_pred = (y_pred_prob &gt; 0.5).astype(int)\n\n            acc = accuracy_score(y_test, y_pred)\n            f1 = f1_score(y_test, y_pred)\n            auc = roc_auc_score(y_test, y_pred_prob)\n\n            print(f\"    Accuracy: {acc:.3f} | F1: {f1:.3f} | AUC: {auc:.3f}\")\n\n            all_results.append({\n                \"method\": label,\n                \"seed\": seed,\n                \"fold\": fold + 1,\n                \"accuracy\": acc,\n                \"f1\": f1,\n                \"auc\": auc\n            })\n\n            del model\n            K.clear_session()\n            gc.collect()\n\n    results_df = pd.DataFrame(all_results)\n    print(f\"\\nüìÑ Saving {label} results to: {out_csv if out_csv else 'not saved (out_csv=None)'}\")\n    print(results_df)\n    if out_csv:\n        results_df.to_csv(out_csv, index=False)\n    return results_df\n\n# === Run and Combine Results ===\nresults_qn = run_rnn_cv(X_padded_qn, y_array_qn, label=\"QN\")\nresults_cb = run_rnn_cv(X_padded_cb, y_array_cb, label=\"ComBat\")\nresults_reg = run_rnn_cv(X_padded_reg, y_array_reg, label=\"Regression\")\n\ncombined_results_df = pd.concat([results_qn, results_cb, results_reg], ignore_index=True)\ncombined_results_df.to_csv(\"rnn_all_methods.csv\", index=False)\nprint(\"‚úÖ Combined RNN results saved to rnn_all_methods.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n=== QN | Seed 2368 ===\n  Fold 1\n\n\n\n\n\n\n\n\n2025-05-14 21:34:59.359381: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n2025-05-14 21:34:59.359443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: g1416ood06.ll.unc.edu\n2025-05-14 21:34:59.359451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: g1416ood06.ll.unc.edu\n2025-05-14 21:34:59.359617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 565.57.1\n2025-05-14 21:34:59.359640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 565.57.1\n2025-05-14 21:34:59.359647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 565.57.1\n\n\n\n\n\n\n\n\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.550\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.497\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.558\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.508\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.500\n\n=== QN | Seed 5272 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.505\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.612\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.679\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.497\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.717\n\n=== QN | Seed 9289 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.500\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.627\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.662\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.503\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.596\n\n=== QN | Seed 1251 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.500\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.657\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.500\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.495\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.663\n\n=== QN | Seed 8825 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.522\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.505\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.492\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.655\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.676\n\n=== QN | Seed 355 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.569\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.716\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.500\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.503\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.678\n\nüìÑ Saving QN results to: not saved (out_csv=None)\n   method  seed  fold  accuracy        f1       auc\n0      QN  2368     1  0.602362  0.751843  0.549893\n1      QN  2368     2  0.602362  0.751843  0.496635\n2      QN  2368     3  0.602362  0.751843  0.558079\n3      QN  2368     4  0.604743  0.753695  0.508137\n4      QN  2368     5  0.600791  0.750617  0.500000\n5      QN  5272     1  0.602362  0.751843  0.504950\n6      QN  5272     2  0.602362  0.751843  0.612308\n7      QN  5272     3  0.602362  0.751843  0.679318\n8      QN  5272     4  0.604743  0.753695  0.496928\n9      QN  5272     5  0.600791  0.750617  0.716747\n10     QN  9289     1  0.602362  0.751843  0.500000\n11     QN  9289     2  0.602362  0.751843  0.626545\n12     QN  9289     3  0.602362  0.751843  0.662104\n13     QN  9289     4  0.604743  0.753695  0.503268\n14     QN  9289     5  0.600791  0.750617  0.595948\n15     QN  1251     1  0.602362  0.751843  0.500000\n16     QN  1251     2  0.602362  0.751843  0.656960\n17     QN  1251     3  0.602362  0.751843  0.500000\n18     QN  1251     4  0.604743  0.753695  0.495000\n19     QN  1251     5  0.600791  0.750617  0.662715\n20     QN  8825     1  0.602362  0.751843  0.521776\n21     QN  8825     2  0.602362  0.751843  0.504950\n22     QN  8825     3  0.602362  0.751843  0.491749\n23     QN  8825     4  0.604743  0.753695  0.654542\n24     QN  8825     5  0.600791  0.750617  0.676394\n25     QN   355     1  0.602362  0.751843  0.568595\n26     QN   355     2  0.602362  0.751843  0.716366\n27     QN   355     3  0.602362  0.751843  0.500000\n28     QN   355     4  0.604743  0.753695  0.503268\n29     QN   355     5  0.600791  0.750617  0.678153\n\n=== ComBat | Seed 2368 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.516\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.490\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.607\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.500\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.505\n\n=== ComBat | Seed 5272 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.492\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.497\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.512\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.498\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.508\n\n=== ComBat | Seed 9289 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.500\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.507\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.500\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.500\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.487\n\n=== ComBat | Seed 1251 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.500\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.500\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.503\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.550\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.500\n\n=== ComBat | Seed 8825 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.500\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.485\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.507\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.517\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.484\n\n=== ComBat | Seed 355 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.505\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.498\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.487\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.534\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.500\n\nüìÑ Saving ComBat results to: not saved (out_csv=None)\n    method  seed  fold  accuracy        f1       auc\n0   ComBat  2368     1  0.602362  0.751843  0.516405\n1   ComBat  2368     2  0.602362  0.751843  0.490099\n2   ComBat  2368     3  0.602362  0.751843  0.607196\n3   ComBat  2368     4  0.604743  0.753695  0.500000\n4   ComBat  2368     5  0.600791  0.750617  0.504950\n5   ComBat  5272     1  0.602362  0.751843  0.491879\n6   ComBat  5272     2  0.602362  0.751843  0.496732\n7   ComBat  5272     3  0.602362  0.751843  0.511616\n8   ComBat  5272     4  0.604743  0.753695  0.498268\n9   ComBat  5272     5  0.600791  0.750617  0.508273\n10  ComBat  9289     1  0.602362  0.751843  0.500000\n11  ComBat  9289     2  0.602362  0.751843  0.506536\n12  ComBat  9289     3  0.602362  0.751843  0.500000\n13  ComBat  9289     4  0.604743  0.753695  0.500000\n14  ComBat  9289     5  0.600791  0.750617  0.486777\n15  ComBat  1251     1  0.602362  0.751843  0.500000\n16  ComBat  1251     2  0.602362  0.751843  0.500000\n17  ComBat  1251     3  0.602362  0.751843  0.503268\n18  ComBat  1251     4  0.604743  0.753695  0.549641\n19  ComBat  1251     5  0.600791  0.750617  0.500000\n20  ComBat  8825     1  0.602362  0.751843  0.500000\n21  ComBat  8825     2  0.602362  0.751843  0.485149\n22  ComBat  8825     3  0.602362  0.751843  0.506536\n23  ComBat  8825     4  0.604743  0.753695  0.516732\n24  ComBat  8825     5  0.600791  0.750617  0.483715\n25  ComBat   355     1  0.602362  0.751843  0.504950\n26  ComBat   355     2  0.602362  0.751843  0.498317\n27  ComBat   355     3  0.602362  0.751843  0.486669\n28  ComBat   355     4  0.604743  0.753695  0.534379\n29  ComBat   355     5  0.600791  0.750617  0.500000\n\n=== Regression | Seed 2368 ===\n  Fold 1\n    Accuracy: 0.583 | F1: 0.651 | AUC: 0.590\n  Fold 2\n    Accuracy: 0.555 | F1: 0.646 | AUC: 0.580\n  Fold 3\n    Accuracy: 0.575 | F1: 0.665 | AUC: 0.593\n  Fold 4\n    Accuracy: 0.569 | F1: 0.663 | AUC: 0.627\n  Fold 5\n    Accuracy: 0.549 | F1: 0.625 | AUC: 0.588\n\n=== Regression | Seed 5272 ===\n  Fold 1\n    Accuracy: 0.563 | F1: 0.641 | AUC: 0.581\n  Fold 2\n    Accuracy: 0.531 | F1: 0.617 | AUC: 0.556\n  Fold 3\n    Accuracy: 0.587 | F1: 0.658 | AUC: 0.607\n  Fold 4\n    Accuracy: 0.581 | F1: 0.649 | AUC: 0.603\n  Fold 5\n    Accuracy: 0.545 | F1: 0.630 | AUC: 0.582\n\n=== Regression | Seed 9289 ===\n  Fold 1\n    Accuracy: 0.591 | F1: 0.658 | AUC: 0.638\n  Fold 2\n    Accuracy: 0.614 | F1: 0.692 | AUC: 0.625\n  Fold 3\n    Accuracy: 0.602 | F1: 0.673 | AUC: 0.631\n  Fold 4\n    Accuracy: 0.621 | F1: 0.698 | AUC: 0.606\n  Fold 5\n    Accuracy: 0.569 | F1: 0.643 | AUC: 0.593\n\n=== Regression | Seed 1251 ===\n  Fold 1\n    Accuracy: 0.567 | F1: 0.645 | AUC: 0.594\n  Fold 2\n    Accuracy: 0.555 | F1: 0.639 | AUC: 0.575\n  Fold 3\n    Accuracy: 0.587 | F1: 0.660 | AUC: 0.619\n  Fold 4\n    Accuracy: 0.597 | F1: 0.669 | AUC: 0.570\n  Fold 5\n    Accuracy: 0.593 | F1: 0.664 | AUC: 0.616\n\n=== Regression | Seed 8825 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.671 | AUC: 0.616\n  Fold 2\n    Accuracy: 0.575 | F1: 0.649 | AUC: 0.587\n  Fold 3\n    Accuracy: 0.520 | F1: 0.596 | AUC: 0.550\n  Fold 4\n    Accuracy: 0.589 | F1: 0.667 | AUC: 0.652\n  Fold 5\n    Accuracy: 0.518 | F1: 0.606 | AUC: 0.538\n\n=== Regression | Seed 355 ===\n  Fold 1\n    Accuracy: 0.618 | F1: 0.690 | AUC: 0.647\n  Fold 2\n    Accuracy: 0.539 | F1: 0.606 | AUC: 0.534\n  Fold 3\n    Accuracy: 0.626 | F1: 0.706 | AUC: 0.627\n  Fold 4\n    Accuracy: 0.593 | F1: 0.669 | AUC: 0.640\n  Fold 5\n    Accuracy: 0.530 | F1: 0.622 | AUC: 0.539\n\nüìÑ Saving Regression results to: not saved (out_csv=None)\n        method  seed  fold  accuracy        f1       auc\n0   Regression  2368     1  0.582677  0.651316  0.589530\n1   Regression  2368     2  0.555118  0.645768  0.580211\n2   Regression  2368     3  0.574803  0.664596  0.592830\n3   Regression  2368     4  0.569170  0.662539  0.627190\n4   Regression  2368     5  0.549407  0.625000  0.587871\n5   Regression  5272     1  0.562992  0.640777  0.581052\n6   Regression  5272     2  0.531496  0.617363  0.555685\n7   Regression  5272     3  0.586614  0.657980  0.607325\n8   Regression  5272     4  0.581028  0.649007  0.603203\n9   Regression  5272     5  0.545455  0.630225  0.582465\n10  Regression  9289     1  0.590551  0.657895  0.637740\n11  Regression  9289     2  0.614173  0.691824  0.625057\n12  Regression  9289     3  0.602362  0.673139  0.630751\n13  Regression  9289     4  0.620553  0.698113  0.606209\n14  Regression  9289     5  0.569170  0.642623  0.593408\n15  Regression  1251     1  0.566929  0.645161  0.594448\n16  Regression  1251     2  0.555118  0.638978  0.575228\n17  Regression  1251     3  0.586614  0.660194  0.619038\n18  Regression  1251     4  0.596838  0.668831  0.570327\n19  Regression  1251     5  0.592885  0.664495  0.616271\n20  Regression  8825     1  0.602362  0.671010  0.616062\n21  Regression  8825     2  0.574803  0.649351  0.586941\n22  Regression  8825     3  0.519685  0.596026  0.550379\n23  Regression  8825     4  0.588933  0.666667  0.652484\n24  Regression  8825     5  0.517787  0.606452  0.537520\n25  Regression   355     1  0.618110  0.690096  0.647382\n26  Regression   355     2  0.539370  0.606061  0.533747\n27  Regression   355     3  0.625984  0.705882  0.627386\n28  Regression   355     4  0.592885  0.668810  0.640131\n29  Regression   355     5  0.529644  0.622222  0.539018\n‚úÖ Combined RNN results saved to rnn_all_methods.csv\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn¬†[10]:\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport gc\nimport tensorflow as tf\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom tensorflow.keras import backend as K\n\n# üîß LSTM Model Builder\ndef build_lstm_model(input_dim):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Masking(mask_value=0.0, input_shape=(None, input_dim)),\n        tf.keras.layers.LSTM(64, return_sequences=False),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\n# üîÅ Cross-Validation Runner\ndef run_lstm_cv(X_padded, y_array, label=\"QN\"):\n    seed_list = [2368, 5272, 9289, 1251, 8825, 355]\n    all_results = []\n\n    for seed in seed_list:\n        print(f\"\\n=== LSTM with {label} | Seed {seed} ===\")\n        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\n        for fold, (train_idx, test_idx) in enumerate(skf.split(X_padded, y_array)):\n            print(f\"  Fold {fold+1}\")\n\n            X_train, X_test = X_padded[train_idx], X_padded[test_idx]\n            y_train, y_test = y_array[train_idx], y_array[test_idx]\n\n            model = build_lstm_model(input_dim=X_padded.shape[2])\n            model.fit(\n                X_train, y_train,\n                epochs=20,\n                batch_size=64,\n                validation_split=0.2,\n                verbose=0\n            )\n\n            y_pred_prob = model.predict(X_test, verbose=0).ravel()\n            y_pred = (y_pred_prob &gt; 0.5).astype(int)\n\n            acc = accuracy_score(y_test, y_pred)\n            f1 = f1_score(y_test, y_pred)\n            auc = roc_auc_score(y_test, y_pred_prob)\n\n            print(f\"    Accuracy: {acc:.3f} | F1: {f1:.3f} | AUC: {auc:.3f}\")\n\n            all_results.append({\n                \"method\": label,\n                \"seed\": seed,\n                \"fold\": fold+1,\n                \"accuracy\": acc,\n                \"f1\": f1,\n                \"auc\": auc\n            })\n\n            del model\n            K.clear_session()\n            gc.collect()\n\n    return pd.DataFrame(all_results)\n\n# üß™ Run LSTM for all normalization methods\nresults_qn  = run_lstm_cv(X_padded_qn, y_array_qn, label=\"QN\")\nresults_cb  = run_lstm_cv(X_padded_cb, y_array_cb, label=\"ComBat\")\nresults_reg = run_lstm_cv(X_padded_reg, y_array_reg, label=\"Regression\")\n\n# üíæ Save results\ncombined_results_df = pd.concat([results_qn, results_cb, results_reg], ignore_index=True)\ncombined_results_df.to_csv(\"lstm_all_methods.csv\", index=False)\nprint(\"‚úÖ LSTM results saved to lstm_all_methods.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n=== LSTM with QN | Seed 2368 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.667\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.733\n  Fold 3\n    Accuracy: 0.669 | F1: 0.768 | AUC: 0.666\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.722\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.797\n\n=== LSTM with QN | Seed 5272 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.715\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.667\n  Fold 3\n    Accuracy: 0.673 | F1: 0.783 | AUC: 0.712\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.737\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.709\n\n=== LSTM with QN | Seed 9289 ===\n  Fold 1\n    Accuracy: 0.709 | F1: 0.786 | AUC: 0.734\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.690\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.722\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.695\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.727\n\n=== LSTM with QN | Seed 1251 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.633\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.696\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.730\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.677\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.741\n\n=== LSTM with QN | Seed 8825 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.742\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.704\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.668\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.650\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.711\n\n=== LSTM with QN | Seed 355 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.713\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.725\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.723\n  Fold 4\n    Accuracy: 0.715 | F1: 0.791 | AUC: 0.742\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.670\n\n=== LSTM with ComBat | Seed 2368 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.625\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.689\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.634\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.668\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.765\n\n=== LSTM with ComBat | Seed 5272 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.702\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.646\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.638\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.693\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.672\n\n=== LSTM with ComBat | Seed 9289 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.723\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.651\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.680\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.646\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.672\n\n=== LSTM with ComBat | Seed 1251 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.697\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.686\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.650\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.645\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.677\n\n=== LSTM with ComBat | Seed 8825 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.677\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.702\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.678\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.687\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.660\n\n=== LSTM with ComBat | Seed 355 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.687\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.681\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.713\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.689\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.601\n\n=== LSTM with Regression | Seed 2368 ===\n  Fold 1\n    Accuracy: 0.638 | F1: 0.721 | AUC: 0.667\n  Fold 2\n    Accuracy: 0.693 | F1: 0.759 | AUC: 0.739\n  Fold 3\n    Accuracy: 0.634 | F1: 0.719 | AUC: 0.656\n  Fold 4\n    Accuracy: 0.700 | F1: 0.758 | AUC: 0.751\n  Fold 5\n    Accuracy: 0.692 | F1: 0.764 | AUC: 0.764\n\n=== LSTM with Regression | Seed 5272 ===\n  Fold 1\n    Accuracy: 0.630 | F1: 0.695 | AUC: 0.703\n  Fold 2\n    Accuracy: 0.618 | F1: 0.727 | AUC: 0.661\n  Fold 3\n    Accuracy: 0.657 | F1: 0.745 | AUC: 0.713\n  Fold 4\n    Accuracy: 0.692 | F1: 0.758 | AUC: 0.729\n  Fold 5\n    Accuracy: 0.636 | F1: 0.721 | AUC: 0.698\n\n=== LSTM with Regression | Seed 9289 ===\n  Fold 1\n    Accuracy: 0.693 | F1: 0.755 | AUC: 0.747\n  Fold 2\n    Accuracy: 0.669 | F1: 0.745 | AUC: 0.694\n  Fold 3\n    Accuracy: 0.642 | F1: 0.718 | AUC: 0.701\n  Fold 4\n    Accuracy: 0.668 | F1: 0.750 | AUC: 0.697\n  Fold 5\n    Accuracy: 0.652 | F1: 0.733 | AUC: 0.709\n\n=== LSTM with Regression | Seed 1251 ===\n  Fold 1\n    Accuracy: 0.677 | F1: 0.747 | AUC: 0.733\n  Fold 2\n    Accuracy: 0.673 | F1: 0.759 | AUC: 0.695\n  Fold 3\n    Accuracy: 0.697 | F1: 0.763 | AUC: 0.727\n  Fold 4\n    Accuracy: 0.640 | F1: 0.720 | AUC: 0.643\n  Fold 5\n    Accuracy: 0.692 | F1: 0.758 | AUC: 0.729\n\n=== LSTM with Regression | Seed 8825 ===\n  Fold 1\n    Accuracy: 0.669 | F1: 0.745 | AUC: 0.708\n  Fold 2\n    Accuracy: 0.669 | F1: 0.751 | AUC: 0.713\n  Fold 3\n    Accuracy: 0.642 | F1: 0.717 | AUC: 0.692\n  Fold 4\n    Accuracy: 0.672 | F1: 0.740 | AUC: 0.709\n  Fold 5\n    Accuracy: 0.696 | F1: 0.762 | AUC: 0.733\n\n=== LSTM with Regression | Seed 355 ===\n  Fold 1\n    Accuracy: 0.697 | F1: 0.751 | AUC: 0.724\n  Fold 2\n    Accuracy: 0.677 | F1: 0.753 | AUC: 0.724\n  Fold 3\n    Accuracy: 0.697 | F1: 0.767 | AUC: 0.723\n  Fold 4\n    Accuracy: 0.664 | F1: 0.745 | AUC: 0.723\n  Fold 5\n    Accuracy: 0.625 | F1: 0.718 | AUC: 0.656\n‚úÖ LSTM results saved to lstm_all_methods.csv\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn¬†[11]:\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport gc\nimport tensorflow as tf\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom tensorflow.keras import backend as K\n\n# üîß GRU Model Builder\ndef build_gru_model(input_dim):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Masking(mask_value=0.0, input_shape=(None, input_dim)),\n        tf.keras.layers.GRU(64, return_sequences=False),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n                  loss='binary_crossentropy', \n                  metrics=['accuracy'])\n    return model\n\n# üîÅ Cross-Validation Runner\ndef run_gru_cv(X_padded, y_array, label=\"QN\"):\n    seed_list = [2368, 5272, 9289, 1251, 8825, 355]\n    all_results = []\n\n    for seed in seed_list:\n        print(f\"\\n=== GRU with {label} | Seed {seed} ===\")\n        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\n        for fold, (train_idx, test_idx) in enumerate(skf.split(X_padded, y_array)):\n            print(f\"  Fold {fold+1}\")\n\n            X_train, X_test = X_padded[train_idx], X_padded[test_idx]\n            y_train, y_test = y_array[train_idx], y_array[test_idx]\n\n            model = build_gru_model(input_dim=X_padded.shape[2])\n\n            model.fit(\n                X_train, y_train,\n                epochs=20,\n                batch_size=64,\n                validation_split=0.2,\n                verbose=0\n            )\n\n            y_pred_prob = model.predict(X_test, verbose=0).ravel()\n            y_pred = (y_pred_prob &gt; 0.5).astype(int)\n\n            acc = accuracy_score(y_test, y_pred)\n            f1 = f1_score(y_test, y_pred)\n            auc = roc_auc_score(y_test, y_pred_prob)\n\n            print(f\"    Accuracy: {acc:.3f} | F1: {f1:.3f} | AUC: {auc:.3f}\")\n\n            all_results.append({\n                \"method\": label,\n                \"seed\": seed,\n                \"fold\": fold+1,\n                \"accuracy\": acc,\n                \"f1\": f1,\n                \"auc\": auc\n            })\n\n            del model\n            K.clear_session()\n            gc.collect()\n\n    return pd.DataFrame(all_results)\n\n# üß™ Run GRU CV for each batch correction method\nresults_qn  = run_gru_cv(X_padded_qn, y_array_qn, label=\"QN\")\nresults_cb  = run_gru_cv(X_padded_cb, y_array_cb, label=\"ComBat\")\nresults_reg = run_gru_cv(X_padded_reg, y_array_reg, label=\"Regression\")\n\n# üíæ Save results\ncombined_results_df = pd.concat([results_qn, results_cb, results_reg], ignore_index=True)\ncombined_results_df.to_csv(\"gru_all_methods.csv\", index=False)\nprint(\"‚úÖ GRU results saved to gru_all_methods.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n=== GRU with QN | Seed 2368 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.647\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.742\n  Fold 3\n    Accuracy: 0.673 | F1: 0.771 | AUC: 0.634\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.754\n  Fold 5\n    Accuracy: 0.711 | F1: 0.787 | AUC: 0.777\n\n=== GRU with QN | Seed 5272 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.750\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.628\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.660\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.670\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.702\n\n=== GRU with QN | Seed 9289 ===\n  Fold 1\n    Accuracy: 0.618 | F1: 0.759 | AUC: 0.756\n  Fold 2\n    Accuracy: 0.650 | F1: 0.756 | AUC: 0.692\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.717\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.678\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.718\n\n=== GRU with QN | Seed 1251 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.671\n  Fold 2\n    Accuracy: 0.673 | F1: 0.761 | AUC: 0.698\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.718\n  Fold 4\n    Accuracy: 0.680 | F1: 0.743 | AUC: 0.708\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.732\n\n=== GRU with QN | Seed 8825 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.721\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.683\n  Fold 3\n    Accuracy: 0.693 | F1: 0.783 | AUC: 0.678\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.655\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.724\n\n=== GRU with QN | Seed 355 ===\n  Fold 1\n    Accuracy: 0.677 | F1: 0.752 | AUC: 0.691\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.703\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.747\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.734\n  Fold 5\n    Accuracy: 0.684 | F1: 0.777 | AUC: 0.682\n\n=== GRU with ComBat | Seed 2368 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.610\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.654\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.621\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.593\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.520\n\n=== GRU with ComBat | Seed 5272 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.675\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.640\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.432\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.658\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.351\n\n=== GRU with ComBat | Seed 9289 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.530\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.588\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.642\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.615\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.632\n\n=== GRU with ComBat | Seed 1251 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.604\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.658\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.628\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.614\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.500\n\n=== GRU with ComBat | Seed 8825 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.619\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.644\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.619\n  Fold 4\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.648\n  Fold 2\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.613\n  Fold 3\n    Accuracy: 0.602 | F1: 0.752 | AUC: 0.360\n  Fold 4\n    Accuracy: 0.605 | F1: 0.754 | AUC: 0.497\n  Fold 5\n    Accuracy: 0.601 | F1: 0.751 | AUC: 0.594\n\n=== GRU with Regression | Seed 2368 ===\n  Fold 1\n    Accuracy: 0.646 | F1: 0.722 | AUC: 0.667\n  Fold 2\n    Accuracy: 0.665 | F1: 0.742 | AUC: 0.698\n  Fold 3\n    Accuracy: 0.630 | F1: 0.719 | AUC: 0.641\n  Fold 4\n    Accuracy: 0.680 | F1: 0.741 | AUC: 0.719\n  Fold 5\n    Accuracy: 0.672 | F1: 0.751 | AUC: 0.715\n\n=== GRU with Regression | Seed 5272 ===\n  Fold 1\n    Accuracy: 0.661 | F1: 0.724 | AUC: 0.679\n  Fold 4\n    Accuracy: 0.656 | F1: 0.732 | AUC: 0.694\n  Fold 5\n    Accuracy: 0.617 | F1: 0.710 | AUC: 0.672\n\n=== GRU with Regression | Seed 1251 ===\n  Fold 1\n    Accuracy: 0.654 | F1: 0.733 | AUC: 0.694\n  Fold 2\n    Accuracy: 0.654 | F1: 0.737 | AUC: 0.662\n  Fold 3\n    Accuracy: 0.665 | F1: 0.738 | AUC: 0.689\n  Fold 4\n    Accuracy: 0.625 | F1: 0.702 | AUC: 0.629\n  Fold 5\n    Accuracy: 0.660 | F1: 0.719 | AUC: 0.696\n\n=== GRU with Regression | Seed 8825 ===\n  Fold 1\n    Accuracy: 0.626 | F1: 0.695 | AUC: 0.670\n  Fold 2\n    Accuracy: 0.630 | F1: 0.719 | AUC: 0.653\n  Fold 3\n    Accuracy: 0.638 | F1: 0.714 | AUC: 0.669\n  Fold 4\n    Accuracy: 0.664 | F1: 0.735 | AUC: 0.724\n  Fold 5\n    Accuracy: 0.648 | F1: 0.721 | AUC: 0.714\n\n=== GRU with Regression | Seed 355 ===\n  Fold 1\n    Accuracy: 0.677 | F1: 0.734 | AUC: 0.708\n  Fold 2\n    Accuracy: 0.685 | F1: 0.745 | AUC: 0.697\n  Fold 3\n    Accuracy: 0.665 | F1: 0.740 | AUC: 0.695\n  Fold 4\n    Accuracy: 0.648 | F1: 0.726 | AUC: 0.727\n  Fold 5\n    Accuracy: 0.593 | F1: 0.693 | AUC: 0.598\n‚úÖ GRU results saved to gru_all_methods.csv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransformer Model: To capture complex temporal patterns and feature interactions in longitudinal gene expression data, we implemented a custom Transformer architecture. The model uses a separate learnable MLP-based time embedding to integrate timepoint information alongside gene expression features. These inputs are combined and passed through a self-attention mechanism (MultiHeadAttention with 4 heads and key dimension 64), followed by global average pooling, dense layers, and dropout for regularization. The final output layer performs binary classification to predict immune response. We trained the model using the Adam optimizer with a learning rate of 1e-4, using a batch size of 64 and 20 training epochs. Stratified 5-fold cross-validation was conducted with six random seeds.\n\n\n\n\n\n\n\n\n\n\n\n\nIn¬†[12]:\n\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_auc_score\nclass TimeEmbedding(layers.Layer):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.mlp = models.Sequential([\n            layers.Dense(embed_dim, activation='relu'),\n            layers.Dense(embed_dim)\n        ])\n\n    def call(self, time_input):\n        return self.mlp(time_input)  # shape: (batch, time, embed_dim)\n\nclass TransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super().__init__()\n        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tf.keras.Sequential([\n            tf.keras.layers.Dense(ff_dim, activation='relu'),\n            tf.keras.layers.Dense(embed_dim)\n        ])\n        self.layernorm1 = tf.keras.layers.LayerNormalization()\n        self.layernorm2 = tf.keras.layers.LayerNormalization()\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training=None):  # üëà Add training=None\n        attn_output = self.att(x, x, training=training)\n        out1 = self.layernorm1(x + self.dropout1(attn_output, training=training))\n        ffn_output = self.ffn(out1)\n        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n    \ndef build_transformer(input_dim, embed_dim=64, num_heads=4, ff_dim=128, max_len=100):\n    gene_input = layers.Input(shape=(None, input_dim))  # (batch, time, gene_features)\n    time_input = layers.Input(shape=(None, 1))           # (batch, time, 1)\n\n    time_embed = TimeEmbedding(embed_dim)(time_input)\n    x = layers.Dense(embed_dim)(gene_input)  # project gene features\n    x = layers.Add()([x, time_embed])        # add time embedding\n\n    x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(64, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    output = layers.Dense(1, activation='sigmoid')(x)\n\n    model = models.Model(inputs=[gene_input, time_input], outputs=output)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn¬†[13]:\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport gc\nimport tensorflow as tf\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom tensorflow.keras import backend as K\n\n# üîß Transformer Model Builder\ndef build_transformer(input_dim):\n    gene_input = tf.keras.Input(shape=(None, input_dim), name='gene_input')\n    time_input = tf.keras.Input(shape=(None, 1), name='time_input')\n\n    x = tf.keras.layers.Concatenate(axis=-1)([gene_input, time_input])\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    x = tf.keras.layers.LayerNormalization()(x)\n    x = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = tf.keras.Model(inputs=[gene_input, time_input], outputs=output)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\n# üîÅ Cross-Validation Runner\ndef run_transformer_cv(X_gene, X_time, y_array, label=\"QN\"):\n    seed_list = [2368, 5272, 9289, 1251, 8825, 355]\n    all_results = []\n\n    for seed in seed_list:\n        print(f\"\\n=== Transformer with {label} | Seed {seed} ===\")\n        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\n        for fold, (train_idx, test_idx) in enumerate(skf.split(X_gene, y_array)):\n            print(f\"  Fold {fold+1}\")\n\n            X_train_gene, X_test_gene = X_gene[train_idx], X_gene[test_idx]\n            X_train_time, X_test_time = X_time[train_idx], X_time[test_idx]\n            y_train, y_test = y_array[train_idx], y_array[test_idx]\n\n            model = build_transformer(input_dim=X_gene.shape[2])\n\n            model.fit(\n                [X_train_gene, X_train_time], y_train,\n                epochs=20,\n                batch_size=64,\n                validation_split=0.2,\n                verbose=0\n            )\n\n            y_pred_prob = model.predict([X_test_gene, X_test_time], verbose=0).ravel()\n            y_pred = (y_pred_prob &gt; 0.5).astype(int)\n\n            acc = accuracy_score(y_test, y_pred)\n            f1 = f1_score(y_test, y_pred)\n            auc = roc_auc_score(y_test, y_pred_prob)\n\n            print(f\"    Accuracy: {acc:.3f} | F1: {f1:.3f} | AUC: {auc:.3f}\")\n\n            all_results.append({\n                \"method\": label,\n                \"seed\": seed,\n                \"fold\": fold+1,\n                \"accuracy\": acc,\n                \"f1\": f1,\n                \"auc\": auc\n            })\n\n            del model\n            K.clear_session()\n            gc.collect()\n\n    return pd.DataFrame(all_results)\n\n# üß™ Run Transformer CV for each normalization method\nG_qn = X_padded_qn.shape[2] - 1\ndf_qn = run_transformer_cv(X_padded_qn[:, :, :G_qn], X_padded_qn[:, :, G_qn:], y_array_qn, label=\"QN\")\n\nG_cb = X_padded_cb.shape[2] - 1\ndf_cb = run_transformer_cv(X_padded_cb[:, :, :G_cb], X_padded_cb[:, :, G_cb:], y_array_cb, label=\"ComBat\")\n\nG_reg = X_padded_reg.shape[2] - 1\ndf_reg = run_transformer_cv(X_padded_reg[:, :, :G_reg], X_padded_reg[:, :, G_reg:], y_array_reg, label=\"Regression\")\n\n# üíæ Save results\ntransformer_results_df = pd.concat([df_qn, df_cb, df_reg], ignore_index=True)\ntransformer_results_df.to_csv(\"transformer_all_methods.csv\", index=False)\nprint(\"‚úÖ Transformer results saved to transformer_all_methods.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n=== Transformer with QN | Seed 2368 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.655 | AUC: 0.671\n  Fold 2\n    Accuracy: 0.669 | F1: 0.698 | AUC: 0.751\n  Fold 3\n    Accuracy: 0.661 | F1: 0.761 | AUC: 0.677\n  Fold 4\n    Accuracy: 0.593 | F1: 0.623 | AUC: 0.724\n  Fold 5\n    Accuracy: 0.731 | F1: 0.795 | AUC: 0.798\n\n=== Transformer with QN | Seed 5272 ===\n  Fold 1\n    Accuracy: 0.665 | F1: 0.704 | AUC: 0.753\n  Fold 2\n    Accuracy: 0.598 | F1: 0.602 | AUC: 0.679\n  Fold 3\n    Accuracy: 0.602 | F1: 0.646 | AUC: 0.715\n  Fold 4\n    Accuracy: 0.632 | F1: 0.649 | AUC: 0.760\n  Fold 5\n    Accuracy: 0.672 | F1: 0.731 | AUC: 0.717\n\n=== Transformer with QN | Seed 9289 ===\n  Fold 1\n    Accuracy: 0.724 | F1: 0.780 | AUC: 0.759\n  Fold 2\n    Accuracy: 0.618 | F1: 0.676 | AUC: 0.702\n  Fold 3\n    Accuracy: 0.646 | F1: 0.720 | AUC: 0.732\n  Fold 4\n    Accuracy: 0.621 | F1: 0.669 | AUC: 0.699\n  Fold 5\n    Accuracy: 0.684 | F1: 0.735 | AUC: 0.742\n\n=== Transformer with QN | Seed 1251 ===\n  Fold 1\n    Accuracy: 0.669 | F1: 0.739 | AUC: 0.721\n  Fold 2\n    Accuracy: 0.650 | F1: 0.721 | AUC: 0.725\n  Fold 3\n    Accuracy: 0.622 | F1: 0.669 | AUC: 0.733\n  Fold 4\n    Accuracy: 0.664 | F1: 0.740 | AUC: 0.696\n  Fold 5\n    Accuracy: 0.664 | F1: 0.693 | AUC: 0.760\n\n=== Transformer with QN | Seed 8825 ===\n  Fold 1\n    Accuracy: 0.717 | F1: 0.789 | AUC: 0.740\n  Fold 2\n    Accuracy: 0.677 | F1: 0.753 | AUC: 0.737\n  Fold 3\n    Accuracy: 0.602 | F1: 0.547 | AUC: 0.725\n  Fold 4\n    Accuracy: 0.644 | F1: 0.662 | AUC: 0.729\n  Fold 5\n    Accuracy: 0.664 | F1: 0.756 | AUC: 0.723\n\n=== Transformer with QN | Seed 355 ===\n  Fold 1\n    Accuracy: 0.669 | F1: 0.739 | AUC: 0.728\n  Fold 2\n    Accuracy: 0.661 | F1: 0.719 | AUC: 0.737\n  Fold 3\n    Accuracy: 0.685 | F1: 0.750 | AUC: 0.740\n  Fold 4\n    Accuracy: 0.711 | F1: 0.764 | AUC: 0.748\n  Fold 5\n    Accuracy: 0.589 | F1: 0.641 | AUC: 0.670\n\n=== Transformer with ComBat | Seed 2368 ===\n  Fold 1\n    Accuracy: 0.614 | F1: 0.671 | AUC: 0.622\n  Fold 2\n    Accuracy: 0.661 | F1: 0.701 | AUC: 0.683\n  Fold 3\n    Accuracy: 0.606 | F1: 0.660 | AUC: 0.648\n  Fold 4\n    Accuracy: 0.589 | F1: 0.620 | AUC: 0.616\n  Fold 5\n    Accuracy: 0.708 | F1: 0.761 | AUC: 0.746\n\n=== Transformer with ComBat | Seed 5272 ===\n  Fold 1\n    Accuracy: 0.665 | F1: 0.710 | AUC: 0.704\n  Fold 2\n    Accuracy: 0.594 | F1: 0.646 | AUC: 0.635\n  Fold 3\n    Accuracy: 0.598 | F1: 0.648 | AUC: 0.631\n  Fold 4\n    Accuracy: 0.660 | F1: 0.705 | AUC: 0.714\n  Fold 5\n    Accuracy: 0.660 | F1: 0.711 | AUC: 0.671\n\n=== Transformer with ComBat | Seed 9289 ===\n  Fold 1\n    Accuracy: 0.673 | F1: 0.722 | AUC: 0.712\n  Fold 2\n    Accuracy: 0.618 | F1: 0.676 | AUC: 0.631\n  Fold 3\n    Accuracy: 0.622 | F1: 0.671 | AUC: 0.703\n  Fold 4\n    Accuracy: 0.617 | F1: 0.667 | AUC: 0.641\n  Fold 5\n    Accuracy: 0.648 | F1: 0.686 | AUC: 0.665\n\n=== Transformer with ComBat | Seed 1251 ===\n  Fold 1\n    Accuracy: 0.630 | F1: 0.685 | AUC: 0.694\n  Fold 2\n    Accuracy: 0.650 | F1: 0.690 | AUC: 0.702\n  Fold 3\n    Accuracy: 0.622 | F1: 0.669 | AUC: 0.610\n  Fold 4\n    Accuracy: 0.609 | F1: 0.673 | AUC: 0.616\n  Fold 5\n    Accuracy: 0.668 | F1: 0.706 | AUC: 0.677\n\n=== Transformer with ComBat | Seed 8825 ===\n  Fold 1\n    Accuracy: 0.630 | F1: 0.669 | AUC: 0.671\n  Fold 2\n    Accuracy: 0.646 | F1: 0.702 | AUC: 0.694\n  Fold 3\n    Accuracy: 0.626 | F1: 0.676 | AUC: 0.675\n  Fold 4\n    Accuracy: 0.668 | F1: 0.710 | AUC: 0.662\n  Fold 5\n    Accuracy: 0.609 | F1: 0.664 | AUC: 0.635\n\n=== Transformer with ComBat | Seed 355 ===\n  Fold 1\n    Accuracy: 0.654 | F1: 0.711 | AUC: 0.684\n  Fold 2\n    Accuracy: 0.634 | F1: 0.680 | AUC: 0.667\n  Fold 3\n    Accuracy: 0.646 | F1: 0.694 | AUC: 0.684\n  Fold 4\n    Accuracy: 0.656 | F1: 0.695 | AUC: 0.674\n  Fold 5\n    Accuracy: 0.589 | F1: 0.641 | AUC: 0.579\n\n=== Transformer with Regression | Seed 2368 ===\n  Fold 1\n    Accuracy: 0.638 | F1: 0.689 | AUC: 0.645\n  Fold 2\n    Accuracy: 0.681 | F1: 0.718 | AUC: 0.704\n  Fold 3\n    Accuracy: 0.579 | F1: 0.637 | AUC: 0.621\n  Fold 4\n    Accuracy: 0.625 | F1: 0.664 | AUC: 0.698\n  Fold 5\n    Accuracy: 0.625 | F1: 0.686 | AUC: 0.664\n\n=== Transformer with Regression | Seed 5272 ===\n  Fold 1\n    Accuracy: 0.575 | F1: 0.606 | AUC: 0.633\n  Fold 2\n    Accuracy: 0.598 | F1: 0.648 | AUC: 0.612\n  Fold 3\n    Accuracy: 0.610 | F1: 0.653 | AUC: 0.650\n  Fold 4\n    Accuracy: 0.561 | F1: 0.608 | AUC: 0.622\n  Fold 5\n    Accuracy: 0.640 | F1: 0.681 | AUC: 0.683\n\n=== Transformer with Regression | Seed 9289 ===\n  Fold 1\n    Accuracy: 0.602 | F1: 0.641 | AUC: 0.657\n  Fold 2\n    Accuracy: 0.610 | F1: 0.635 | AUC: 0.648\n  Fold 3\n    Accuracy: 0.626 | F1: 0.669 | AUC: 0.675\n  Fold 4\n    Accuracy: 0.565 | F1: 0.610 | AUC: 0.598\n  Fold 5\n    Accuracy: 0.605 | F1: 0.650 | AUC: 0.661\n\n=== Transformer with Regression | Seed 1251 ===\n  Fold 1\n    Accuracy: 0.634 | F1: 0.678 | AUC: 0.696\n  Fold 2\n    Accuracy: 0.622 | F1: 0.698 | AUC: 0.666\n  Fold 3\n    Accuracy: 0.618 | F1: 0.660 | AUC: 0.654\n  Fold 4\n    Accuracy: 0.601 | F1: 0.662 | AUC: 0.599\n  Fold 5\n    Accuracy: 0.621 | F1: 0.631 | AUC: 0.671\n\n=== Transformer with Regression | Seed 8825 ===\n  Fold 1\n    Accuracy: 0.606 | F1: 0.621 | AUC: 0.663\n  Fold 2\n    Accuracy: 0.606 | F1: 0.673 | AUC: 0.640\n  Fold 3\n    Accuracy: 0.602 | F1: 0.658 | AUC: 0.629\n  Fold 4\n    Accuracy: 0.621 | F1: 0.671 | AUC: 0.666\n  Fold 5\n    Accuracy: 0.593 | F1: 0.636 | AUC: 0.649\n\n=== Transformer with Regression | Seed 355 ===\n  Fold 1\n    Accuracy: 0.610 | F1: 0.657 | AUC: 0.656\n  Fold 2\n    Accuracy: 0.594 | F1: 0.639 | AUC: 0.644\n  Fold 3\n    Accuracy: 0.669 | F1: 0.718 | AUC: 0.696\n  Fold 4\n    Accuracy: 0.640 | F1: 0.685 | AUC: 0.678\n  Fold 5\n    Accuracy: 0.553 | F1: 0.614 | AUC: 0.568\n‚úÖ Transformer results saved to transformer_all_methods.csv\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn¬†[14]:\n\n\n\n\n# Read individual model result CSVs\ndf_transformer = pd.read_csv(\"transformer_all_methods.csv\")\ndf_gru        = pd.read_csv(\"gru_all_methods.csv\")\ndf_lstm       = pd.read_csv(\"lstm_all_methods.csv\")\ndf_rnn        = pd.read_csv(\"rnn_all_methods.csv\")\ndf_logistic   = pd.read_csv(\"logistic_regression_results.csv\")\n\n# Add model labels\ndf_transformer['model'] = 'Transformer'\ndf_gru['model']         = 'GRU'\ndf_lstm['model']        = 'LSTM'\ndf_rnn['model']         = 'RNN'\ndf_logistic['model']    = 'PCA_Logistic'\n\n# Combine all into one DataFrame\ncombined_df = pd.concat(\n    [df_transformer, df_gru, df_lstm, df_rnn, df_logistic],\n    ignore_index=True\n)\n\n# Display preview\ncombined_df.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut[14]:\n\n\n\n\n\n\n\n\n\n\nmethod\n\n\nseed\n\n\nfold\n\n\naccuracy\n\n\nf1\n\n\nauc\n\n\nmodel\n\n\n\n\n\n\n0\n\n\nQN\n\n\n2368\n\n\n1\n\n\n0.602362\n\n\n0.655290\n\n\n0.671002\n\n\nTransformer\n\n\n\n\n1\n\n\nQN\n\n\n2368\n\n\n2\n\n\n0.669291\n\n\n0.697842\n\n\n0.750857\n\n\nTransformer\n\n\n\n\n2\n\n\nQN\n\n\n2368\n\n\n3\n\n\n0.661417\n\n\n0.761111\n\n\n0.677085\n\n\nTransformer\n\n\n\n\n3\n\n\nQN\n\n\n2368\n\n\n4\n\n\n0.592885\n\n\n0.622711\n\n\n0.724118\n\n\nTransformer\n\n\n\n\n4\n\n\nQN\n\n\n2368\n\n\n5\n\n\n0.731225\n\n\n0.795181\n\n\n0.797876\n\n\nTransformer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn¬†[¬†]:\n\n\n\n\n# Save the combined DataFrame to a CSV file\ncombined_df.to_csv(\"combined_model_results.csv\", index=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn¬†[¬†]:\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Melt the DataFrame\nplot_df = combined_df.melt(\n    id_vars=['method', 'model'],\n    value_vars=['f1', 'accuracy', 'auc'],\n    var_name='Metric',\n    value_name='Score'\n)\n\n# Set model order (must match 'model' column values exactly)\nmodel_order = ['PCA_Logistic', 'RNN', 'LSTM', 'GRU', 'Transformer']\n\n# Custom color palette\ncustom_palette = ['#87CEEB', '#009ACD', '#003F5C']\n\n# === AUC Plot (with legend) ===\nmetric = 'auc'\ndf_metric = plot_df[plot_df['Metric'] == metric]\n\nplt.figure(figsize=(17, 10))\nsns.barplot(\n    data=df_metric,\n    x='model', y='Score', hue='method',\n    ci=95,\n    palette=custom_palette,\n    order=model_order,\n    errorbar='sd'\n)\n\nplt.title('AUC', fontsize=30)\nplt.xlabel('Model', fontsize=30)\nplt.ylabel('AUC Score', fontsize=30)\nplt.xticks(rotation=45, ha='right', fontsize=25)\nplt.yticks(fontsize=25)\nplt.ylim(0, 1)\nplt.legend(\n    title='Batch Method', title_fontsize=22, fontsize=20,\n    loc='center left',\n    bbox_to_anchor=(1.05, 0.5),\n    borderaxespad=0\n)\n\nplt.tight_layout()\nplt.savefig(\"auc_plot.png\", dpi=300)\nplt.show()\n\n# === F1 and Accuracy Plots (no legend) ===\nmetrics_to_plot = ['f1', 'accuracy']\ntitles = {'f1': 'F1 Score', 'accuracy': 'Accuracy'}\n\nfor metric in metrics_to_plot:\n    df_metric = plot_df[plot_df['Metric'] == metric]\n\n    plt.figure(figsize=(14, 10))\n    sns.barplot(\n        data=df_metric,\n        x='model', y='Score', hue='method',\n        ci=95,\n        palette=custom_palette,\n        order=model_order,\n        errorbar='sd'\n    )\n\n    plt.title(titles[metric], fontsize=30)\n    plt.xlabel('Model', fontsize=30)\n    plt.ylabel(f'{titles[metric]}', fontsize=30)\n    plt.xticks(rotation=45, ha='right', fontsize=25)\n    plt.yticks(fontsize=25)\n    plt.ylim(0, 1)\n    plt.legend([], [], frameon=False)\n    plt.tight_layout()\n    plt.savefig(f\"{metric}_plot.png\", dpi=300)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/tmp/ipykernel_1299954/1799928948.py:24: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=('ci', 95)` for the same effect.\n\n  sns.barplot(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/tmp/ipykernel_1299954/1799928948.py:58: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=('ci', 95)` for the same effect.\n\n  sns.barplot(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/tmp/ipykernel_1299954/1799928948.py:58: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=('ci', 95)` for the same effect.\n\n  sns.barplot(",
    "crumbs": [
      "Home",
      "Outcome Prediction Using Longitudinal Bulk Data",
      "Antibody Responder Prediction"
    ]
  }
]